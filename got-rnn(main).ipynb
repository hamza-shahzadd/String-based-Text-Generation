{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18443,"sourceType":"datasetVersion","datasetId":13668}],"dockerImageVersionId":30028,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://static.posters.cz/image/750/poster/il-trono-di-spade-game-of-thrones-logo-i21034.jpg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 20, 6\nrcParams['axes.grid'] = True\n\ndf = [pd.read_json(f'../input/game-of-thrones-srt/season{i+1}.json') for i in range(7)]\ndf = pd.concat(df, axis=1)\nprint(df.shape)\ndf.head()\n\n\ndf = pd.DataFrame(df.values.reshape(-1, 1), columns=['Script']).dropna().reset_index(drop=True)\ndf.head()\n\nimport re\ncleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n\ndef cleanhtml(raw_html):\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\ndf['Script'] = df['Script'].apply(cleanhtml)\n\ntext = '\\n'.join(df['Script'].values)\nvocab = sorted(set(text))\nprint('{} unique characters'.format(len(vocab)))\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])\n\nimport tensorflow as tf\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])\n    \nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))\n    \ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)\n\nfor input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset\n\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = ['G1024']\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import plot_model\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n    for rnn_unit in rnn_units:\n        layer_type = rnn_unit[0]\n        num_cells = int(rnn_unit[1:])\n        if layer_type == 'G':\n            model.add(GRU(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n        elif layer_type == 'L':\n            model.add(LSTM(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n        else:\n            model.add(SimpleRNN(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n\n    model.add(Dense(vocab_size))\n    return model\n\nmodel = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE\n)\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n    \nsampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n\n\nsampled_indices\n\nprint(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n\n\nmodel.compile(optimizer='adam', loss=loss)\nEPOCHS = 500\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[es, ckpt, rlp])\npd.DataFrame(history.history)[['loss']].plot();\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(filepath)\nmodel.build(tf.TensorShape([1, None]))\nmodel.summary()\n\ndef generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))\nprint(generate_text(model, start_string=\"Seven Hells\"))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:11:46.206970Z","iopub.execute_input":"2024-05-07T12:11:46.207341Z","iopub.status.idle":"2024-05-07T12:46:18.292781Z","shell.execute_reply.started":"2024-05-07T12:11:46.207306Z","shell.execute_reply":"2024-05-07T12:46:18.291840Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(983, 68)\n86 unique characters\nE\na\ns\ny\n,\n\"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\n' Ser Hugh have any family in the capital?\\nYour pardon, Your Grace.\\n\"Summoned to court to answer for t'\n\"he crimes\\nYah! Left high, left low.\\nYou've seen better days, my lord.\\nLook at me. Look at me!\\nWell st\"\n\"ruck.\\nGotta be ready before nightfall.\\nOut, all of you.\\nIt's got to be the Mountain. He's the biggest\"\n\".\\n- You swear it? - By the mother.\\nI've taken your castle.\\nA cripple?\\n- Riders approaching! - Open th\"\nInput data:  \"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoe\"\nTarget data: \"asy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\nStep    0\n  input: 32 ('E')\n  expected output: 56 ('a')\nStep    1\n  input: 56 ('a')\n  expected output: 74 ('s')\nStep    2\n  input: 74 ('s')\n  expected output: 80 ('y')\nStep    3\n  input: 80 ('y')\n  expected output: 9 (',')\nStep    4\n  input: 9 (',')\n  expected output: 1 (' ')\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           22016     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 86)            88150     \n=================================================================\nTotal params: 4,048,470\nTrainable params: 4,048,470\nNon-trainable params: 0\n_________________________________________________________________\n(64, 100, 86) # (batch_size, sequence_length, vocab_size)\nInput: \n \"gned the year we met.\\nIf the climbers get too high, drop the scythe on them.\\nHang on!\\nStonespear?\\nI'\"\n\nNext Char Predictions: \n 'g♪bP1-6hTDdh@Yhe@unx0?@H@VZTz@|tHg\\nC>g.Oq?23Zu;h2,xlmh0J,g?ñ b@r(:NnuzruFOfy\"I)l`=b:Zm,Mf\"zKLnCs=ksQ'\nPrediction shape:  (64, 100, 86)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.45362\nEpoch 1/500\n232/232 [==============================] - ETA: 0s - loss: 2.4480\nEpoch 00001: loss improved from inf to 2.44800, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 2.4480\nEpoch 2/500\n231/232 [============================>.] - ETA: 0s - loss: 1.7260\nEpoch 00002: loss improved from 2.44800 to 1.72529, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.7253\nEpoch 3/500\n232/232 [==============================] - ETA: 0s - loss: 1.4598\nEpoch 00003: loss improved from 1.72529 to 1.45976, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.4598\nEpoch 4/500\n231/232 [============================>.] - ETA: 0s - loss: 1.3410\nEpoch 00004: loss improved from 1.45976 to 1.34065, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.3406\nEpoch 5/500\n231/232 [============================>.] - ETA: 0s - loss: 1.2744\nEpoch 00005: loss improved from 1.34065 to 1.27429, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.2743\nEpoch 6/500\n231/232 [============================>.] - ETA: 0s - loss: 1.2279\nEpoch 00006: loss improved from 1.27429 to 1.22793, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.2279\nEpoch 7/500\n232/232 [==============================] - ETA: 0s - loss: 1.1903\nEpoch 00007: loss improved from 1.22793 to 1.19025, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.1903\nEpoch 8/500\n231/232 [============================>.] - ETA: 0s - loss: 1.1593\nEpoch 00008: loss improved from 1.19025 to 1.15927, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.1593\nEpoch 9/500\n231/232 [============================>.] - ETA: 0s - loss: 1.1290\nEpoch 00009: loss improved from 1.15927 to 1.12883, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.1288\nEpoch 10/500\n231/232 [============================>.] - ETA: 0s - loss: 1.1016\nEpoch 00010: loss improved from 1.12883 to 1.10171, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.1017\nEpoch 11/500\n232/232 [==============================] - ETA: 0s - loss: 1.0743\nEpoch 00011: loss improved from 1.10171 to 1.07427, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.0743\nEpoch 12/500\n232/232 [==============================] - ETA: 0s - loss: 1.0471\nEpoch 00012: loss improved from 1.07427 to 1.04710, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.0471\nEpoch 13/500\n231/232 [============================>.] - ETA: 0s - loss: 1.0207\nEpoch 00013: loss improved from 1.04710 to 1.02086, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 1.0209\nEpoch 14/500\n232/232 [==============================] - ETA: 0s - loss: 0.9945\nEpoch 00014: loss improved from 1.02086 to 0.99455, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.9945\nEpoch 15/500\n231/232 [============================>.] - ETA: 0s - loss: 0.9676\nEpoch 00015: loss improved from 0.99455 to 0.96771, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.9677\nEpoch 16/500\n231/232 [============================>.] - ETA: 0s - loss: 0.9429\nEpoch 00016: loss improved from 0.96771 to 0.94294, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.9429\nEpoch 17/500\n231/232 [============================>.] - ETA: 0s - loss: 0.9184\nEpoch 00017: loss improved from 0.94294 to 0.91850, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.9185\nEpoch 18/500\n232/232 [==============================] - ETA: 0s - loss: 0.8950\nEpoch 00018: loss improved from 0.91850 to 0.89500, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8950\nEpoch 19/500\n231/232 [============================>.] - ETA: 0s - loss: 0.8732\nEpoch 00019: loss improved from 0.89500 to 0.87328, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8733\nEpoch 20/500\n231/232 [============================>.] - ETA: 0s - loss: 0.8537\nEpoch 00020: loss improved from 0.87328 to 0.85388, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8539\nEpoch 21/500\n231/232 [============================>.] - ETA: 0s - loss: 0.8356\nEpoch 00021: loss improved from 0.85388 to 0.83571, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8357\nEpoch 22/500\n231/232 [============================>.] - ETA: 0s - loss: 0.8218\nEpoch 00022: loss improved from 0.83571 to 0.82202, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8220\nEpoch 23/500\n231/232 [============================>.] - ETA: 0s - loss: 0.8070\nEpoch 00023: loss improved from 0.82202 to 0.80718, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.8072\nEpoch 24/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7943\nEpoch 00024: loss improved from 0.80718 to 0.79455, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7946\nEpoch 25/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7845\nEpoch 00025: loss improved from 0.79455 to 0.78476, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7848\nEpoch 26/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7767\nEpoch 00026: loss improved from 0.78476 to 0.77684, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7768\nEpoch 27/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7688\nEpoch 00027: loss improved from 0.77684 to 0.76893, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7689\nEpoch 28/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7643\nEpoch 00028: loss improved from 0.76893 to 0.76448, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7645\nEpoch 29/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7585\nEpoch 00029: loss improved from 0.76448 to 0.75859, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7586\nEpoch 30/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7545\nEpoch 00030: loss improved from 0.75859 to 0.75469, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7547\nEpoch 31/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7524\nEpoch 00031: loss improved from 0.75469 to 0.75263, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7526\nEpoch 32/500\n232/232 [==============================] - ETA: 0s - loss: 0.7491\nEpoch 00032: loss improved from 0.75263 to 0.74913, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7491\nEpoch 33/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7476\nEpoch 00033: loss improved from 0.74913 to 0.74780, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7478\nEpoch 34/500\n232/232 [==============================] - ETA: 0s - loss: 0.7472\nEpoch 00034: loss improved from 0.74780 to 0.74721, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.7472\nEpoch 35/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7482\nEpoch 00035: loss did not improve from 0.74721\n232/232 [==============================] - 10s 42ms/step - loss: 0.7484\nEpoch 36/500\n232/232 [==============================] - ETA: 0s - loss: 0.7480\nEpoch 00036: loss did not improve from 0.74721\n232/232 [==============================] - 10s 42ms/step - loss: 0.7480\nEpoch 37/500\n231/232 [============================>.] - ETA: 0s - loss: 0.7483\nEpoch 00037: loss did not improve from 0.74721\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n232/232 [==============================] - 10s 42ms/step - loss: 0.7485\nEpoch 38/500\n231/232 [============================>.] - ETA: 0s - loss: 0.6515\nEpoch 00038: loss improved from 0.74721 to 0.65147, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.6515\nEpoch 39/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5928\nEpoch 00039: loss improved from 0.65147 to 0.59291, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5929\nEpoch 40/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5662\nEpoch 00040: loss improved from 0.59291 to 0.56625, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5663\nEpoch 41/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5465\nEpoch 00041: loss improved from 0.56625 to 0.54650, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5465\nEpoch 42/500\n232/232 [==============================] - ETA: 0s - loss: 0.5328\nEpoch 00042: loss improved from 0.54650 to 0.53285, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5328\nEpoch 43/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5202\nEpoch 00043: loss improved from 0.53285 to 0.52017, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5202\nEpoch 44/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5106\nEpoch 00044: loss improved from 0.52017 to 0.51054, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5105\nEpoch 45/500\n231/232 [============================>.] - ETA: 0s - loss: 0.5017\nEpoch 00045: loss improved from 0.51054 to 0.50172, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.5017\nEpoch 46/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4941\nEpoch 00046: loss improved from 0.50172 to 0.49416, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4942\nEpoch 47/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4871\nEpoch 00047: loss improved from 0.49416 to 0.48709, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4871\nEpoch 48/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4798\nEpoch 00048: loss improved from 0.48709 to 0.47981, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4798\nEpoch 49/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4740\nEpoch 00049: loss improved from 0.47981 to 0.47407, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4741\nEpoch 50/500\n232/232 [==============================] - ETA: 0s - loss: 0.4666\nEpoch 00050: loss improved from 0.47407 to 0.46660, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4666\nEpoch 51/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4631\nEpoch 00051: loss improved from 0.46660 to 0.46315, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4631\nEpoch 52/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4563\nEpoch 00052: loss improved from 0.46315 to 0.45636, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4564\nEpoch 53/500\n232/232 [==============================] - ETA: 0s - loss: 0.4528\nEpoch 00053: loss improved from 0.45636 to 0.45276, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4528\nEpoch 54/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4485\nEpoch 00054: loss improved from 0.45276 to 0.44855, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4486\nEpoch 55/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4439\nEpoch 00055: loss improved from 0.44855 to 0.44390, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4439\nEpoch 56/500\n232/232 [==============================] - ETA: 0s - loss: 0.4400\nEpoch 00056: loss improved from 0.44390 to 0.43997, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4400\nEpoch 57/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4361\nEpoch 00057: loss improved from 0.43997 to 0.43611, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4361\nEpoch 58/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4322\nEpoch 00058: loss improved from 0.43611 to 0.43224, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4322\nEpoch 59/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4292\nEpoch 00059: loss improved from 0.43224 to 0.42928, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4293\nEpoch 60/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4259\nEpoch 00060: loss improved from 0.42928 to 0.42586, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4259\nEpoch 61/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4230\nEpoch 00061: loss improved from 0.42586 to 0.42303, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4230\nEpoch 62/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4204\nEpoch 00062: loss improved from 0.42303 to 0.42037, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4204\nEpoch 63/500\n232/232 [==============================] - ETA: 0s - loss: 0.4164\nEpoch 00063: loss improved from 0.42037 to 0.41637, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4164\nEpoch 64/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4147\nEpoch 00064: loss improved from 0.41637 to 0.41478, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4148\nEpoch 65/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4120\nEpoch 00065: loss improved from 0.41478 to 0.41202, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4120\nEpoch 66/500\n232/232 [==============================] - ETA: 0s - loss: 0.4098\nEpoch 00066: loss improved from 0.41202 to 0.40981, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4098\nEpoch 67/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4067\nEpoch 00067: loss improved from 0.40981 to 0.40672, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4067\nEpoch 68/500\n232/232 [==============================] - ETA: 0s - loss: 0.4048\nEpoch 00068: loss improved from 0.40672 to 0.40477, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4048\nEpoch 69/500\n231/232 [============================>.] - ETA: 0s - loss: 0.4028\nEpoch 00069: loss improved from 0.40477 to 0.40281, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.4028\nEpoch 70/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3997\nEpoch 00070: loss improved from 0.40281 to 0.39970, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3997\nEpoch 71/500\n232/232 [==============================] - ETA: 0s - loss: 0.3983\nEpoch 00071: loss improved from 0.39970 to 0.39834, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3983\nEpoch 72/500\n232/232 [==============================] - ETA: 0s - loss: 0.3959\nEpoch 00072: loss improved from 0.39834 to 0.39590, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3959\nEpoch 73/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3948\nEpoch 00073: loss improved from 0.39590 to 0.39472, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3947\nEpoch 74/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3932\nEpoch 00074: loss improved from 0.39472 to 0.39321, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3932\nEpoch 75/500\n232/232 [==============================] - ETA: 0s - loss: 0.3908\nEpoch 00075: loss improved from 0.39321 to 0.39083, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3908\nEpoch 76/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3884\nEpoch 00076: loss improved from 0.39083 to 0.38840, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3884\nEpoch 77/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3873\nEpoch 00077: loss improved from 0.38840 to 0.38731, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3873\nEpoch 78/500\n232/232 [==============================] - ETA: 0s - loss: 0.3860\nEpoch 00078: loss improved from 0.38731 to 0.38600, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3860\nEpoch 79/500\n232/232 [==============================] - ETA: 0s - loss: 0.3837\nEpoch 00079: loss improved from 0.38600 to 0.38365, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3837\nEpoch 80/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3830\nEpoch 00080: loss improved from 0.38365 to 0.38305, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3830\nEpoch 81/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3816\nEpoch 00081: loss improved from 0.38305 to 0.38155, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3816\nEpoch 82/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3797\nEpoch 00082: loss improved from 0.38155 to 0.37970, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3797\nEpoch 83/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3792\nEpoch 00083: loss improved from 0.37970 to 0.37919, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3792\nEpoch 84/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3769\nEpoch 00084: loss improved from 0.37919 to 0.37686, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3769\nEpoch 85/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3756\nEpoch 00085: loss improved from 0.37686 to 0.37567, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3757\nEpoch 86/500\n232/232 [==============================] - ETA: 0s - loss: 0.3739\nEpoch 00086: loss improved from 0.37567 to 0.37390, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3739\nEpoch 87/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3737\nEpoch 00087: loss improved from 0.37390 to 0.37373, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3737\nEpoch 88/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3719\nEpoch 00088: loss improved from 0.37373 to 0.37195, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3719\nEpoch 89/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3705\nEpoch 00089: loss improved from 0.37195 to 0.37051, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3705\nEpoch 90/500\n232/232 [==============================] - ETA: 0s - loss: 0.3690\nEpoch 00090: loss improved from 0.37051 to 0.36897, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3690\nEpoch 91/500\n232/232 [==============================] - ETA: 0s - loss: 0.3679\nEpoch 00091: loss improved from 0.36897 to 0.36787, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3679\nEpoch 92/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3680\nEpoch 00092: loss improved from 0.36787 to 0.36786, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3679\nEpoch 93/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3661\nEpoch 00093: loss improved from 0.36786 to 0.36611, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3661\nEpoch 94/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3649\nEpoch 00094: loss improved from 0.36611 to 0.36494, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3649\nEpoch 95/500\n232/232 [==============================] - ETA: 0s - loss: 0.3647\nEpoch 00095: loss improved from 0.36494 to 0.36466, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3647\nEpoch 96/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3636\nEpoch 00096: loss improved from 0.36466 to 0.36351, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3635\nEpoch 97/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3619\nEpoch 00097: loss improved from 0.36351 to 0.36189, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3619\nEpoch 98/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3618\nEpoch 00098: loss improved from 0.36189 to 0.36183, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3618\nEpoch 99/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3609\nEpoch 00099: loss improved from 0.36183 to 0.36091, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3609\nEpoch 100/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3598\nEpoch 00100: loss improved from 0.36091 to 0.35984, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3598\nEpoch 101/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3582\nEpoch 00101: loss improved from 0.35984 to 0.35828, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3583\nEpoch 102/500\n232/232 [==============================] - ETA: 0s - loss: 0.3580\nEpoch 00102: loss improved from 0.35828 to 0.35796, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3580\nEpoch 103/500\n232/232 [==============================] - ETA: 0s - loss: 0.3568\nEpoch 00103: loss improved from 0.35796 to 0.35681, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3568\nEpoch 104/500\n232/232 [==============================] - ETA: 0s - loss: 0.3555\nEpoch 00104: loss improved from 0.35681 to 0.35552, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3555\nEpoch 105/500\n232/232 [==============================] - ETA: 0s - loss: 0.3552\nEpoch 00105: loss improved from 0.35552 to 0.35523, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3552\nEpoch 106/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3544\nEpoch 00106: loss improved from 0.35523 to 0.35443, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3544\nEpoch 107/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3537\nEpoch 00107: loss improved from 0.35443 to 0.35374, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3537\nEpoch 108/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3536\nEpoch 00108: loss improved from 0.35374 to 0.35365, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3537\nEpoch 109/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3523\nEpoch 00109: loss improved from 0.35365 to 0.35228, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3523\nEpoch 110/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3514\nEpoch 00110: loss improved from 0.35228 to 0.35146, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3515\nEpoch 111/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3505\nEpoch 00111: loss improved from 0.35146 to 0.35056, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3506\nEpoch 112/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3509\nEpoch 00112: loss did not improve from 0.35056\n232/232 [==============================] - 10s 42ms/step - loss: 0.3509\nEpoch 113/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3488\nEpoch 00113: loss improved from 0.35056 to 0.34888, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3489\nEpoch 114/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3490\nEpoch 00114: loss did not improve from 0.34888\n232/232 [==============================] - 10s 42ms/step - loss: 0.3490\nEpoch 115/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3469\nEpoch 00115: loss improved from 0.34888 to 0.34695, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3470\nEpoch 116/500\n232/232 [==============================] - ETA: 0s - loss: 0.3488\nEpoch 00116: loss did not improve from 0.34695\n232/232 [==============================] - 10s 42ms/step - loss: 0.3488\nEpoch 117/500\n232/232 [==============================] - ETA: 0s - loss: 0.3462\nEpoch 00117: loss improved from 0.34695 to 0.34623, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3462\nEpoch 118/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3461\nEpoch 00118: loss improved from 0.34623 to 0.34608, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3461\nEpoch 119/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3464\nEpoch 00119: loss did not improve from 0.34608\n232/232 [==============================] - 10s 42ms/step - loss: 0.3464\nEpoch 120/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3447\nEpoch 00120: loss improved from 0.34608 to 0.34476, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3448\nEpoch 121/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3449\nEpoch 00121: loss did not improve from 0.34476\n232/232 [==============================] - 10s 42ms/step - loss: 0.3449\nEpoch 122/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3435\nEpoch 00122: loss improved from 0.34476 to 0.34357, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3436\nEpoch 123/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3426\nEpoch 00123: loss improved from 0.34357 to 0.34257, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3426\nEpoch 124/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3426\nEpoch 00124: loss did not improve from 0.34257\n232/232 [==============================] - 10s 43ms/step - loss: 0.3426\nEpoch 125/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3409\nEpoch 00125: loss improved from 0.34257 to 0.34080, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3408\nEpoch 126/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3410\nEpoch 00126: loss did not improve from 0.34080\n232/232 [==============================] - 10s 43ms/step - loss: 0.3410\nEpoch 127/500\n232/232 [==============================] - ETA: 0s - loss: 0.3410\nEpoch 00127: loss did not improve from 0.34080\n232/232 [==============================] - 10s 42ms/step - loss: 0.3410\nEpoch 128/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3398\nEpoch 00128: loss improved from 0.34080 to 0.33988, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3399\nEpoch 129/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3395\nEpoch 00129: loss improved from 0.33988 to 0.33948, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3395\nEpoch 130/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3389\nEpoch 00130: loss improved from 0.33948 to 0.33888, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3389\nEpoch 131/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3387\nEpoch 00131: loss improved from 0.33888 to 0.33878, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3388\nEpoch 132/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3388\nEpoch 00132: loss did not improve from 0.33878\n232/232 [==============================] - 10s 43ms/step - loss: 0.3388\nEpoch 133/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3372\nEpoch 00133: loss improved from 0.33878 to 0.33723, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3372\nEpoch 134/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3373\nEpoch 00134: loss did not improve from 0.33723\n232/232 [==============================] - 10s 43ms/step - loss: 0.3374\nEpoch 135/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3372\nEpoch 00135: loss did not improve from 0.33723\n232/232 [==============================] - 10s 43ms/step - loss: 0.3373\nEpoch 136/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3361\nEpoch 00136: loss improved from 0.33723 to 0.33615, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3362\nEpoch 137/500\n232/232 [==============================] - ETA: 0s - loss: 0.3362\nEpoch 00137: loss did not improve from 0.33615\n232/232 [==============================] - 10s 43ms/step - loss: 0.3362\nEpoch 138/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3358\nEpoch 00138: loss improved from 0.33615 to 0.33579, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.3358\nEpoch 139/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3343\nEpoch 00139: loss improved from 0.33579 to 0.33437, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3344\nEpoch 140/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3344\nEpoch 00140: loss did not improve from 0.33437\n232/232 [==============================] - 10s 43ms/step - loss: 0.3344\nEpoch 141/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3333\nEpoch 00141: loss improved from 0.33437 to 0.33328, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3333\nEpoch 142/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3334\nEpoch 00142: loss did not improve from 0.33328\n232/232 [==============================] - 10s 43ms/step - loss: 0.3333\nEpoch 143/500\n232/232 [==============================] - ETA: 0s - loss: 0.3327\nEpoch 00143: loss improved from 0.33328 to 0.33267, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3327\nEpoch 144/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3317\nEpoch 00144: loss improved from 0.33267 to 0.33168, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3317\nEpoch 145/500\n232/232 [==============================] - ETA: 0s - loss: 0.3319\nEpoch 00145: loss did not improve from 0.33168\n232/232 [==============================] - 10s 43ms/step - loss: 0.3319\nEpoch 146/500\n232/232 [==============================] - ETA: 0s - loss: 0.3316\nEpoch 00146: loss improved from 0.33168 to 0.33160, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3316\nEpoch 147/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3306\nEpoch 00147: loss improved from 0.33160 to 0.33069, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3307\nEpoch 148/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3310\nEpoch 00148: loss did not improve from 0.33069\n232/232 [==============================] - 10s 43ms/step - loss: 0.3310\nEpoch 149/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3298\nEpoch 00149: loss improved from 0.33069 to 0.32979, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3298\nEpoch 150/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3289\nEpoch 00150: loss improved from 0.32979 to 0.32887, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3289\nEpoch 151/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3301\nEpoch 00151: loss did not improve from 0.32887\n232/232 [==============================] - 10s 43ms/step - loss: 0.3301\nEpoch 152/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3287\nEpoch 00152: loss improved from 0.32887 to 0.32881, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3288\nEpoch 153/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3292\nEpoch 00153: loss did not improve from 0.32881\n\nEpoch 00153: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n232/232 [==============================] - 10s 43ms/step - loss: 0.3292\nEpoch 154/500\n232/232 [==============================] - ETA: 0s - loss: 0.3223\nEpoch 00154: loss improved from 0.32881 to 0.32231, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3223\nEpoch 155/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3202\nEpoch 00155: loss improved from 0.32231 to 0.32021, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3202\nEpoch 156/500\n232/232 [==============================] - ETA: 0s - loss: 0.3189\nEpoch 00156: loss improved from 0.32021 to 0.31886, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3189\nEpoch 157/500\n232/232 [==============================] - ETA: 0s - loss: 0.3182\nEpoch 00157: loss improved from 0.31886 to 0.31821, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3182\nEpoch 158/500\n232/232 [==============================] - ETA: 0s - loss: 0.3171\nEpoch 00158: loss improved from 0.31821 to 0.31711, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3171\nEpoch 159/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3164\nEpoch 00159: loss improved from 0.31711 to 0.31645, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3164\nEpoch 160/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3172\nEpoch 00160: loss did not improve from 0.31645\n232/232 [==============================] - 10s 43ms/step - loss: 0.3171\nEpoch 161/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3167\nEpoch 00161: loss did not improve from 0.31645\n232/232 [==============================] - 10s 42ms/step - loss: 0.3168\nEpoch 162/500\n232/232 [==============================] - ETA: 0s - loss: 0.3163\nEpoch 00162: loss improved from 0.31645 to 0.31627, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3163\nEpoch 163/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3151\nEpoch 00163: loss improved from 0.31627 to 0.31504, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3150\nEpoch 164/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3143\nEpoch 00164: loss improved from 0.31504 to 0.31433, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3143\nEpoch 165/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3156\nEpoch 00165: loss did not improve from 0.31433\n232/232 [==============================] - 10s 43ms/step - loss: 0.3157\nEpoch 166/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3152\nEpoch 00166: loss did not improve from 0.31433\n232/232 [==============================] - 10s 43ms/step - loss: 0.3153\nEpoch 167/500\n232/232 [==============================] - ETA: 0s - loss: 0.3152\nEpoch 00167: loss did not improve from 0.31433\n\nEpoch 00167: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3152\nEpoch 168/500\n232/232 [==============================] - ETA: 0s - loss: 0.3148\nEpoch 00168: loss did not improve from 0.31433\n232/232 [==============================] - 10s 42ms/step - loss: 0.3148\nEpoch 169/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3140\nEpoch 00169: loss improved from 0.31433 to 0.31409, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3141\nEpoch 170/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3144\nEpoch 00170: loss did not improve from 0.31409\n232/232 [==============================] - 10s 42ms/step - loss: 0.3144\nEpoch 171/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3137\nEpoch 00171: loss improved from 0.31409 to 0.31371, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3137\nEpoch 172/500\n232/232 [==============================] - ETA: 0s - loss: 0.3132\nEpoch 00172: loss improved from 0.31371 to 0.31324, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.3132\nEpoch 173/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3138\nEpoch 00173: loss did not improve from 0.31324\n232/232 [==============================] - 10s 42ms/step - loss: 0.3138\nEpoch 174/500\n232/232 [==============================] - ETA: 0s - loss: 0.3141\nEpoch 00174: loss did not improve from 0.31324\n232/232 [==============================] - 10s 42ms/step - loss: 0.3141\nEpoch 175/500\n232/232 [==============================] - ETA: 0s - loss: 0.3143\nEpoch 00175: loss did not improve from 0.31324\n\nEpoch 00175: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3143\nEpoch 176/500\n232/232 [==============================] - ETA: 0s - loss: 0.3142\nEpoch 00176: loss did not improve from 0.31324\n232/232 [==============================] - 10s 42ms/step - loss: 0.3142\nEpoch 177/500\n232/232 [==============================] - ETA: 0s - loss: 0.3135\nEpoch 00177: loss did not improve from 0.31324\n232/232 [==============================] - 10s 42ms/step - loss: 0.3135\nEpoch 178/500\n232/232 [==============================] - ETA: 0s - loss: 0.3135\nEpoch 00178: loss did not improve from 0.31324\n\nEpoch 00178: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3135\nEpoch 179/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3126\nEpoch 00179: loss improved from 0.31324 to 0.31265, saving model to model.h5\n232/232 [==============================] - 10s 42ms/step - loss: 0.3127\nEpoch 180/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3128\nEpoch 00180: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3128\nEpoch 181/500\n232/232 [==============================] - ETA: 0s - loss: 0.3134\nEpoch 00181: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3134\nEpoch 182/500\n232/232 [==============================] - ETA: 0s - loss: 0.3133\nEpoch 00182: loss did not improve from 0.31265\n\nEpoch 00182: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3133\nEpoch 183/500\n232/232 [==============================] - ETA: 0s - loss: 0.3138\nEpoch 00183: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3138\nEpoch 184/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3137\nEpoch 00184: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3137\nEpoch 185/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3139\nEpoch 00185: loss did not improve from 0.31265\n\nEpoch 00185: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3140\nEpoch 186/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3138\nEpoch 00186: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3139\nEpoch 187/500\n232/232 [==============================] - ETA: 0s - loss: 0.3139\nEpoch 00187: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3139\nEpoch 188/500\n232/232 [==============================] - ETA: 0s - loss: 0.3128\nEpoch 00188: loss did not improve from 0.31265\n\nEpoch 00188: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n232/232 [==============================] - 10s 42ms/step - loss: 0.3128\nEpoch 189/500\n231/232 [============================>.] - ETA: 0s - loss: 0.3137\nEpoch 00189: loss did not improve from 0.31265\n232/232 [==============================] - 10s 42ms/step - loss: 0.3137\nEpoch 00189: early stopping\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            22016     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 86)             88150     \n=================================================================\nTotal params: 4,048,470\nTrainable params: 4,048,470\nNon-trainable params: 0\n_________________________________________________________________\nSeven Hells!\nPlease take all of Dragons: You see in frandsome flower.\nSer Jaime, I'm not afraid.\nWhere I come from, thank you.\nWhen Jon Snow returns with the wildlings, though the innocents will be the new many of them we hold.\nHave you heard a difference between shut your mother and the find Lord Petyr Baelish, ser.\n- I want to. They need st.\nWell, he's nothing more than mel nothing,\nThey're the first our door.\nHold the door!\nYou can risk your home, my lady.\nNow, I have a trial's Daughter.\nQueen Margaery!\nWe're transporting any further.\nEving men are not my enemy.\none day,\nBut it doesn't matter now. He's dead,\nWhen attacked the Wall.\nYour family at least.\nAnd we took those cands and the King's son.\nYou understand me?\nready to dear myself.\nthrew too much revenge. We need allies?\nThen of his name.\nI remember what it feels like.\nIt's a long way to King's Landing. They won't go any fathers,\nWho needs to be done. - What?\n- You're not from wry.\nForgive me, Khaleesi, but you don't know.\nGone, along wit\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]}]}