{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18443,"sourceType":"datasetVersion","datasetId":13668}],"dockerImageVersionId":30028,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://static.posters.cz/image/750/poster/il-trono-di-spade-game-of-thrones-logo-i21034.jpg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 20, 6\nrcParams['axes.grid'] = True\n\ndf = [pd.read_json(f'../input/game-of-thrones-srt/season{i+1}.json') for i in range(7)]\ndf = pd.concat(df, axis=1)\nprint(df.shape)\ndf.head()\n\ndf = pd.DataFrame(df.values.reshape(-1, 1), columns=['Script']).dropna().reset_index(drop=True)\ndf.head()\n\nimport re\ncleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n\ndef cleanhtml(raw_html):\n  cleantext = re.sub(cleanr, '', raw_html)\n  return cleantext\n\ndf['Script'] = df['Script'].apply(cleanhtml)\n\ntext = '\\n'.join(df['Script'].values)\nvocab = sorted(set(text))\nprint('{} unique characters'.format(len(vocab)))\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])\n\nimport tensorflow as tf\n\n\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])\n    \n    \nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))\n    \ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)\n\nfor input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n    \nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n    \nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset\n\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = ['G1024']\n\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import plot_model\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size=None):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n    for rnn_unit in rnn_units:\n        layer_type = rnn_unit[0]\n        num_cells = int(rnn_unit[1:])\n        if layer_type == 'G':\n            model.add(GRU(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n        elif layer_type == 'L':\n            model.add(LSTM(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n        else:\n            model.add(SimpleRNN(\n                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n            ))\n\n    model.add(Dense(vocab_size))\n    return model\n\n\nmodel = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=64\n)\n\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n\nsampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n\nsampled_indices\n\nprint(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\ndef accuracy(labels, logits):\n    predicted_ids = tf.argmax(logits, axis=-1)\n    predicted_ids = tf.cast(predicted_ids, tf.int32)  # Cast logits to int32\n    labels = tf.cast(labels, tf.int32)  # Add this line to cast labels to int32\n    correct_predictions = tf.equal(predicted_ids, labels)\n    return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n\n\n\nexample_batch_loss = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n\n# Compile the model with loss and accuracy metrics\nmodel.compile(optimizer='adam', loss=loss, metrics=[accuracy])\n\n\nEPOCHS = 50\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[es, ckpt, rlp])\npd.DataFrame(history.history)[['loss']].plot();\n\nlearning_rates = [0.001, 0.01, 0.1]  # Example learning rates to experiment with\n\nhistories = []\n\nfor lr in learning_rates:\n    # Rebuild the model\n    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss=loss, metrics=[accuracy])\n    \n    # Train the model\n    history = model.fit(dataset, epochs=EPOCHS, callbacks=[es, ckpt, rlp], verbose=0)\n    \n    histories.append(history)\n\n# Plot the loss and accuracy curves for each learning rate\nplt.figure(figsize=(12, 8))\nfor i, lr in enumerate(learning_rates):\n    plt.subplot(2, 1, 1)\n    plt.plot(histories[i].history['loss'], label='lr={}'.format(lr))\n    plt.title('Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(2, 1, 2)\n    plt.plot(histories[i].history['accuracy'], label='lr={}'.format(lr))\n    plt.title('Training Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(filepath)\nmodel.build(tf.TensorShape([1, None]))\nmodel.summary()\n\n\ndef generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))\n\nprint(generate_text(model, start_string=\"Hells\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:40:18.822318Z","iopub.execute_input":"2024-05-07T10:40:18.822692Z","iopub.status.idle":"2024-05-07T10:49:50.071358Z","shell.execute_reply.started":"2024-05-07T10:40:18.822661Z","shell.execute_reply":"2024-05-07T10:49:50.068196Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(983, 68)\n86 unique characters\nE\na\ns\ny\n,\n\"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\n' Ser Hugh have any family in the capital?\\nYour pardon, Your Grace.\\n\"Summoned to court to answer for t'\n\"he crimes\\nYah! Left high, left low.\\nYou've seen better days, my lord.\\nLook at me. Look at me!\\nWell st\"\n\"ruck.\\nGotta be ready before nightfall.\\nOut, all of you.\\nIt's got to be the Mountain. He's the biggest\"\n\".\\n- You swear it? - By the mother.\\nI've taken your castle.\\nA cripple?\\n- Riders approaching! - Open th\"\nInput data:  \"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoe\"\nTarget data: \"asy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\nStep    0\n  input: 32 ('E')\n  expected output: 56 ('a')\nStep    1\n  input: 56 ('a')\n  expected output: 74 ('s')\nStep    2\n  input: 74 ('s')\n  expected output: 80 ('y')\nStep    3\n  input: 80 ('y')\n  expected output: 9 (',')\nStep    4\n  input: 9 (',')\n  expected output: 1 (' ')\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           22016     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 86)            88150     \n=================================================================\nTotal params: 4,048,470\nTrainable params: 4,048,470\nNon-trainable params: 0\n_________________________________________________________________\n(64, 100, 86) # (batch_size, sequence_length, vocab_size)\nInput: \n \"ath.\\nBut, Your Grace, this is my laboratory.\\nso cowards in masks could take it away.\\nThat's a good b\"\n\nNext Char Predictions: \n 'yQrQLLM:CWyJrE#zuñl=?1oU:IEFHJrC.8íiHNXd`jODp:y!|LeOG\"(S\\ns;.RIm>vD>\\nTí2-4!1_ imi8S@2M=_T X\\n|E609F-Sq'\nPrediction shape:  (64, 100, 86)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.4535427\nEpoch 1/50\n231/232 [============================>.] - ETA: 0s - loss: 2.4700 - accuracy: 0.3372\nEpoch 00001: loss improved from inf to 2.46768, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 2.4677 - accuracy: 0.3376\nEpoch 2/50\n231/232 [============================>.] - ETA: 0s - loss: 1.7397 - accuracy: 0.4896\nEpoch 00002: loss improved from 2.46768 to 1.73888, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.7389 - accuracy: 0.4898\nEpoch 3/50\n232/232 [==============================] - ETA: 0s - loss: 1.4666 - accuracy: 0.5617\nEpoch 00003: loss improved from 1.73888 to 1.46660, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.4666 - accuracy: 0.5617\nEpoch 4/50\n231/232 [============================>.] - ETA: 0s - loss: 1.3447 - accuracy: 0.5935\nEpoch 00004: loss improved from 1.46660 to 1.34453, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.3445 - accuracy: 0.5936\nEpoch 5/50\n232/232 [==============================] - ETA: 0s - loss: 1.2758 - accuracy: 0.6113\nEpoch 00005: loss improved from 1.34453 to 1.27575, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.2758 - accuracy: 0.6113\nEpoch 6/50\n232/232 [==============================] - ETA: 0s - loss: 1.2297 - accuracy: 0.6237\nEpoch 00006: loss improved from 1.27575 to 1.22970, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 1.2297 - accuracy: 0.6237\nEpoch 7/50\n231/232 [============================>.] - ETA: 0s - loss: 1.1924 - accuracy: 0.6336\nEpoch 00007: loss improved from 1.22970 to 1.19234, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.1923 - accuracy: 0.6336\nEpoch 8/50\n231/232 [============================>.] - ETA: 0s - loss: 1.1610 - accuracy: 0.6423\nEpoch 00008: loss improved from 1.19234 to 1.16095, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.1610 - accuracy: 0.6423\nEpoch 9/50\n231/232 [============================>.] - ETA: 0s - loss: 1.1322 - accuracy: 0.6501\nEpoch 00009: loss improved from 1.16095 to 1.13241, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 1.1324 - accuracy: 0.6501\nEpoch 10/50\n232/232 [==============================] - ETA: 0s - loss: 1.1043 - accuracy: 0.6580\nEpoch 00010: loss improved from 1.13241 to 1.10427, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.1043 - accuracy: 0.6580\nEpoch 11/50\n232/232 [==============================] - ETA: 0s - loss: 1.0769 - accuracy: 0.6665\nEpoch 00011: loss improved from 1.10427 to 1.07692, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.0769 - accuracy: 0.6665\nEpoch 12/50\n231/232 [============================>.] - ETA: 0s - loss: 1.0512 - accuracy: 0.6740\nEpoch 00012: loss improved from 1.07692 to 1.05137, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.0514 - accuracy: 0.6740\nEpoch 13/50\n231/232 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.6822\nEpoch 00013: loss improved from 1.05137 to 1.02465, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 1.0247 - accuracy: 0.6821\nEpoch 14/50\n231/232 [============================>.] - ETA: 0s - loss: 0.9981 - accuracy: 0.6905\nEpoch 00014: loss improved from 1.02465 to 0.99824, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.9982 - accuracy: 0.6904\nEpoch 15/50\n231/232 [============================>.] - ETA: 0s - loss: 0.9723 - accuracy: 0.6987\nEpoch 00015: loss improved from 0.99824 to 0.97243, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.9724 - accuracy: 0.6987\nEpoch 16/50\n231/232 [============================>.] - ETA: 0s - loss: 0.9476 - accuracy: 0.7068\nEpoch 00016: loss improved from 0.97243 to 0.94773, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.9477 - accuracy: 0.7068\nEpoch 17/50\n231/232 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.7152\nEpoch 00017: loss improved from 0.94773 to 0.92254, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.9225 - accuracy: 0.7152\nEpoch 18/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8998 - accuracy: 0.7227\nEpoch 00018: loss improved from 0.92254 to 0.90004, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.9000 - accuracy: 0.7226\nEpoch 19/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8783 - accuracy: 0.7301\nEpoch 00019: loss improved from 0.90004 to 0.87851, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.8785 - accuracy: 0.7300\nEpoch 20/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8585 - accuracy: 0.7364\nEpoch 00020: loss improved from 0.87851 to 0.85856, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.8586 - accuracy: 0.7364\nEpoch 21/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8400 - accuracy: 0.7430\nEpoch 00021: loss improved from 0.85856 to 0.84016, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.8402 - accuracy: 0.7430\nEpoch 22/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8231 - accuracy: 0.7486\nEpoch 00022: loss improved from 0.84016 to 0.82332, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.8233 - accuracy: 0.7486\nEpoch 23/50\n231/232 [============================>.] - ETA: 0s - loss: 0.8100 - accuracy: 0.7534\nEpoch 00023: loss improved from 0.82332 to 0.81011, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.8101 - accuracy: 0.7534\nEpoch 24/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7978 - accuracy: 0.7575\nEpoch 00024: loss improved from 0.81011 to 0.79806, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7981 - accuracy: 0.7574\nEpoch 25/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7864 - accuracy: 0.7612\nEpoch 00025: loss improved from 0.79806 to 0.78667, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.7867 - accuracy: 0.7612\nEpoch 26/50\n232/232 [==============================] - ETA: 0s - loss: 0.7771 - accuracy: 0.7646\nEpoch 00026: loss improved from 0.78667 to 0.77714, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7771 - accuracy: 0.7646\nEpoch 27/50\n232/232 [==============================] - ETA: 0s - loss: 0.7697 - accuracy: 0.7668\nEpoch 00027: loss improved from 0.77714 to 0.76974, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7697 - accuracy: 0.7668\nEpoch 28/50\n232/232 [==============================] - ETA: 0s - loss: 0.7640 - accuracy: 0.7684\nEpoch 00028: loss improved from 0.76974 to 0.76397, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7640 - accuracy: 0.7684\nEpoch 29/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7583 - accuracy: 0.7709\nEpoch 00029: loss improved from 0.76397 to 0.75839, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7584 - accuracy: 0.7708\nEpoch 30/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7533 - accuracy: 0.7724\nEpoch 00030: loss improved from 0.75839 to 0.75342, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7534 - accuracy: 0.7723\nEpoch 31/50\n232/232 [==============================] - ETA: 0s - loss: 0.7503 - accuracy: 0.7734\nEpoch 00031: loss improved from 0.75342 to 0.75034, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7503 - accuracy: 0.7734\nEpoch 32/50\n232/232 [==============================] - ETA: 0s - loss: 0.7490 - accuracy: 0.7738\nEpoch 00032: loss improved from 0.75034 to 0.74897, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7490 - accuracy: 0.7738\nEpoch 33/50\n232/232 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.7748\nEpoch 00033: loss improved from 0.74897 to 0.74553, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7455 - accuracy: 0.7748\nEpoch 34/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7454 - accuracy: 0.7746\nEpoch 00034: loss did not improve from 0.74553\n232/232 [==============================] - 10s 43ms/step - loss: 0.7456 - accuracy: 0.7746\nEpoch 35/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7454 - accuracy: 0.7748\nEpoch 00035: loss improved from 0.74553 to 0.74551, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7455 - accuracy: 0.7747\nEpoch 36/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7443 - accuracy: 0.7749\nEpoch 00036: loss improved from 0.74551 to 0.74437, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.7444 - accuracy: 0.7749\nEpoch 37/50\n232/232 [==============================] - ETA: 0s - loss: 0.7449 - accuracy: 0.7747\nEpoch 00037: loss did not improve from 0.74437\n232/232 [==============================] - 10s 44ms/step - loss: 0.7449 - accuracy: 0.7747\nEpoch 38/50\n231/232 [============================>.] - ETA: 0s - loss: 0.7470 - accuracy: 0.7734\nEpoch 00038: loss did not improve from 0.74437\n232/232 [==============================] - 10s 43ms/step - loss: 0.7472 - accuracy: 0.7734\nEpoch 39/50\n232/232 [==============================] - ETA: 0s - loss: 0.7469 - accuracy: 0.7739\nEpoch 00039: loss did not improve from 0.74437\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n232/232 [==============================] - 10s 43ms/step - loss: 0.7469 - accuracy: 0.7739\nEpoch 40/50\n232/232 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.8120\nEpoch 00040: loss improved from 0.74437 to 0.65189, saving model to model.h5\n232/232 [==============================] - 10s 44ms/step - loss: 0.6519 - accuracy: 0.8120\nEpoch 41/50\n231/232 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.8387\nEpoch 00041: loss improved from 0.65189 to 0.59198, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5920 - accuracy: 0.8387\nEpoch 42/50\n231/232 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.8516\nEpoch 00042: loss improved from 0.59198 to 0.56385, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5638 - accuracy: 0.8516\nEpoch 43/50\n232/232 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.8601\nEpoch 00043: loss improved from 0.56385 to 0.54579, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5458 - accuracy: 0.8601\nEpoch 44/50\n232/232 [==============================] - ETA: 0s - loss: 0.5319 - accuracy: 0.8667\nEpoch 00044: loss improved from 0.54579 to 0.53186, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5319 - accuracy: 0.8667\nEpoch 45/50\n231/232 [============================>.] - ETA: 0s - loss: 0.5197 - accuracy: 0.8716\nEpoch 00045: loss improved from 0.53186 to 0.51967, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5197 - accuracy: 0.8716\nEpoch 46/50\n231/232 [============================>.] - ETA: 0s - loss: 0.5097 - accuracy: 0.8762\nEpoch 00046: loss improved from 0.51967 to 0.50980, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5098 - accuracy: 0.8762\nEpoch 47/50\n231/232 [============================>.] - ETA: 0s - loss: 0.5010 - accuracy: 0.8801\nEpoch 00047: loss improved from 0.50980 to 0.50097, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.5010 - accuracy: 0.8801\nEpoch 48/50\n231/232 [============================>.] - ETA: 0s - loss: 0.4927 - accuracy: 0.8837\nEpoch 00048: loss improved from 0.50097 to 0.49266, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.4927 - accuracy: 0.8837\nEpoch 49/50\n231/232 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.8866\nEpoch 00049: loss improved from 0.49266 to 0.48544, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.4854 - accuracy: 0.8866\nEpoch 50/50\n232/232 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.8895\nEpoch 00050: loss improved from 0.48544 to 0.47834, saving model to model.h5\n232/232 [==============================] - 10s 43ms/step - loss: 0.4783 - accuracy: 0.8895\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-398deed9a3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:659 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:227 assert_input_compatibility\n        ', found shape=' + str(shape))\n\n    ValueError: Input 0 is incompatible with layer gru_1: expected shape=(1, None, 256), found shape=[64, 100, 256]\n"],"ename":"ValueError","evalue":"in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:659 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:227 assert_input_compatibility\n        ', found shape=' + str(shape))\n\n    ValueError: Input 0 is incompatible with layer gru_1: expected shape=(1, None, 256), found shape=[64, 100, 256]\n","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]}]}