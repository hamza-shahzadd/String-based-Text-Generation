{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1311864,"sourceType":"datasetVersion","datasetId":759820}],"dockerImageVersionId":29995,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport time\n\n#check if decoding is needed: text may need to be decoded as utf-8\ntext = open('/kaggle/input/shakespeare-text/text.txt', 'r').read() \nprint(text[:200])\n#Find Vocabulary (set of characters)\nvocabulary = sorted(set(text))\nprint('No. of unique characters: {}'.format(len(vocabulary)))\n#character to index mapping\nchar2index = {c:i for i,c in enumerate(vocabulary)}\nint_text = np.array([char2index[i] for i in text])\n\n#Index to character mapping\nindex2char = np.array(vocabulary)\n\n#Testing\nprint(\"Character to Index: \\n\")\nfor char,_ in zip(char2index, range(65)):\n    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n\nprint(\"\\nInput text to Integer: \\n\")\nprint('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging\n\nseq_length= 150 #max number of characters that can be fed as a single input\nexamples_per_epoch = len(text)\n\n#converts text (vector) into character index stream\n#Reference: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(int_text)\n\n#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\n#Testing\nprint(\"Character Stream: \\n\")\nfor i in char_dataset.take(10):\n  print(index2char[i.numpy()])  \n\nprint(\"\\nSequence: \\n\")\nfor i in sequences.take(10):\n  print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it\n\ndef create_input_target_pair(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(create_input_target_pair)\n\n#Testing\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(index2char[target_example.numpy()])))\n    \n#Creating batches\n\nBATCH_SIZE = 64\n\n# Buffer used to shuffle the dataset \n# Reference: https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset\n\nvocab_size = len(vocabulary)\nembedding_dim = 256\nrnn_units= 1024\n\n\ndef build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units, \n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model\n\n# Reference for theory: https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/\n\nlstm_model = build_model_lstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\n\n#Testing: shape\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_prediction = lstm_model(input_example_batch)\n    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n    #print(example_prediction.shape)\n    \nsampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n#Loss Function reference: https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/\n\nexample_loss  = loss(target_example_batch, example_prediction)\nprint(\"Prediction shape: \", example_prediction.shape)\nprint(\"Loss:      \", example_loss.numpy().mean())\n\nlstm_model.compile(optimizer='adam', loss=loss)\n\nlstm_dir_checkpoints= './training_checkpoints_LSTM'\ncheckpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)\n\n\nEPOCHS=50 #increase number of epochs for better results (lesser loss)\n\nhistory = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n\ntf.train.latest_checkpoint(lstm_dir_checkpoints)\n\nlstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\nlstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\nlstm_model.build(tf.TensorShape([1, None]))\n\nlstm_model.summary()\n\ndef generate_text(model, start_string):\n    num_generate = 1000 #Number of characters to be generated\n\n    input_eval = [char2index[s] for s in start_string] #vectorising input\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(index2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))\n\n#Prediction with User Input\nlstm_test = input(\"Enter your starting string: \")\nprint(generate_text(lstm_model, start_string=lstm_test))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T11:55:05.991736Z","iopub.execute_input":"2024-05-07T11:55:05.992073Z","iopub.status.idle":"2024-05-07T12:10:31.258384Z","shell.execute_reply.started":"2024-05-07T11:55:05.992043Z","shell.execute_reply":"2024-05-07T12:10:31.257501Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/shakespeare-text/text.txt\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you\nNo. of unique characters: 65\nCharacter to Index: \n\n  '\\n':   0\n  ' ' :   1\n  '!' :   2\n  '$' :   3\n  '&' :   4\n  \"'\" :   5\n  ',' :   6\n  '-' :   7\n  '.' :   8\n  '3' :   9\n  ':' :  10\n  ';' :  11\n  '?' :  12\n  'A' :  13\n  'B' :  14\n  'C' :  15\n  'D' :  16\n  'E' :  17\n  'F' :  18\n  'G' :  19\n  'H' :  20\n  'I' :  21\n  'J' :  22\n  'K' :  23\n  'L' :  24\n  'M' :  25\n  'N' :  26\n  'O' :  27\n  'P' :  28\n  'Q' :  29\n  'R' :  30\n  'S' :  31\n  'T' :  32\n  'U' :  33\n  'V' :  34\n  'W' :  35\n  'X' :  36\n  'Y' :  37\n  'Z' :  38\n  'a' :  39\n  'b' :  40\n  'c' :  41\n  'd' :  42\n  'e' :  43\n  'f' :  44\n  'g' :  45\n  'h' :  46\n  'i' :  47\n  'j' :  48\n  'k' :  49\n  'l' :  50\n  'm' :  51\n  'n' :  52\n  'o' :  53\n  'p' :  54\n  'q' :  55\n  'r' :  56\n  's' :  57\n  't' :  58\n  'u' :  59\n  'v' :  60\n  'w' :  61\n  'x' :  62\n  'y' :  63\n  'z' :  64\n\nInput text to Integer: \n\n'First Citizen:\\nBefor' mapped to [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\nCharacter Stream: \n\nF\ni\nr\ns\nt\n \nC\ni\nt\ni\n\nSequence: \n\n'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n\"l:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n\"ill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good \"\n'citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but '\n'the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the '\n'object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we '\n'become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caiu'\n\"s Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst \"\n'Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not m'\n'aliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was'\nInput data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\nPrediction shape:  (64, 150, 65)\nLoss:       4.175309\nEpoch 1/50\n115/115 [==============================] - 9s 78ms/step - loss: 2.9704\nEpoch 2/50\n115/115 [==============================] - 9s 77ms/step - loss: 2.2387\nEpoch 3/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.9975\nEpoch 4/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.8210\nEpoch 5/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.6924\nEpoch 6/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.6025\nEpoch 7/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.5322\nEpoch 8/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.4809\nEpoch 9/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.4389\nEpoch 10/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.4044\nEpoch 11/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.3751\nEpoch 12/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.3492\nEpoch 13/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.3278\nEpoch 14/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.3079\nEpoch 15/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.2892\nEpoch 16/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.2711\nEpoch 17/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.2544\nEpoch 18/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.2388\nEpoch 19/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.2227\nEpoch 20/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.2082\nEpoch 21/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.1925\nEpoch 22/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.1763\nEpoch 23/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.1613\nEpoch 24/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.1442\nEpoch 25/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.1285\nEpoch 26/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.1113\nEpoch 27/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.0940\nEpoch 28/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.0778\nEpoch 29/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.0581\nEpoch 30/50\n115/115 [==============================] - 9s 77ms/step - loss: 1.0388\nEpoch 31/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.0213\nEpoch 32/50\n115/115 [==============================] - 9s 76ms/step - loss: 1.0014\nEpoch 33/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.9798\nEpoch 34/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.9603\nEpoch 35/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.9386\nEpoch 36/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.9176\nEpoch 37/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.8947\nEpoch 38/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.8726\nEpoch 39/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.8511\nEpoch 40/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.8304\nEpoch 41/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.8083\nEpoch 42/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.7858\nEpoch 43/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.7655\nEpoch 44/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.7459\nEpoch 45/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.7241\nEpoch 46/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.7052\nEpoch 47/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.6862\nEpoch 48/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.6689\nEpoch 49/50\n115/115 [==============================] - 9s 77ms/step - loss: 0.6502\nEpoch 50/50\n115/115 [==============================] - 9s 76ms/step - loss: 0.6339\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\nlstm_3 (LSTM)                (1, None, 1024)           5246976   \n_________________________________________________________________\ndense_3 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 5,330,241\nTrainable params: 5,330,241\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your starting string:  romeo\n"},{"name":"stdout","text":"romeos cannot oppress the\ncourtesy, and they are croop'dou do so increasy\nThan can my banisher.\n\nPETRUCHIO:\nWhose word is this forlight story to be,\nThe late of England, there they mut and bid\nThe priest is not in the mutiate of the house,\nWhen I have might have hid more resolved with\nthe sharp to the angel, which may make her the deed\nAs throne and holds upon him as the morn;\nFor she is secure a stranger, and most accursed\nAnd all the justice of your mistress ere have done\nBut darkness in this place of hell his works it is.\n\nNORTHUMBERLAND:\nNor I.\n\nCLARENCE:\nWe have anoney bore me?\n\nBIONDELLO:\nWhen we breathed depated the morning dried.\nAnd, here's no more of them all; then she may, and such as you\nHave the nightingale: it was too sudden'd with the night,\nWhen calm death in heaven.\n\nHENRY BOLINGBROKE:\nCousin, stand from me, sir; and therefore fire\nPriers of the mind to play at the goose?\n\nMERCUTIO:\nWhy, sir, I spake like not of many of mine: then, as I love myself,\nThat I might steal and c\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}